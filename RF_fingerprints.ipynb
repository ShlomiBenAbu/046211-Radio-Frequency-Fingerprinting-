{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "id": "BXyVK7IE3fvS",
    "ExecuteTime": {
     "end_time": "2025-01-24T18:29:00.241478Z",
     "start_time": "2025-01-24T18:28:53.746978Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset Preparation\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, file_list, idx, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for file in file_list:\n",
    "            with h5py.File(file, 'r') as f:\n",
    "                self.data.append(f['X'][:][idx])\n",
    "                self.labels.append(f['Y'][:][idx])\n",
    "        self.data = np.vstack(self.data)\n",
    "        self.labels = np.vstack(self.labels)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ],
   "metadata": {
    "id": "8GzkY0OW3kmO",
    "ExecuteTime": {
     "end_time": "2025-01-24T18:29:05.991717Z",
     "start_time": "2025-01-24T18:29:05.986096Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Transformer Components\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\"Embedding dimension must be divisible by number of heads\")\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        query = self.query(x).view(batch_size, seq_len, self.num_heads, self.projection_dim).transpose(1, 2)\n",
    "        key = self.key(x).view(batch_size, seq_len, self.num_heads, self.projection_dim).transpose(1, 2)\n",
    "        value = self.value(x).view(batch_size, seq_len, self.num_heads, self.projection_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.projection_dim)\n",
    "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        attention = torch.matmul(weights, value)\n",
    "\n",
    "        concat_attention = attention.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ],
   "metadata": {
    "id": "a4O15FmW4fyT",
    "ExecuteTime": {
     "end_time": "2025-01-24T18:29:13.373104Z",
     "start_time": "2025-01-24T18:29:13.366731Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        out1 = self.layernorm1(x + self.dropout1(attn_output))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + self.dropout2(ffn_output))"
   ],
   "metadata": {
    "id": "XnIe8Lj64ljm",
    "ExecuteTime": {
     "end_time": "2025-01-24T18:29:18.331972Z",
     "start_time": "2025-01-24T18:29:18.326727Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Model Definition\n",
    "class ProposedModel(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=1024, num_heads=128, ff_dim=256):\n",
    "        super(ProposedModel, self).__init__()\n",
    "        self.reshape = nn.Linear(input_shape[-1], embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.AlphaDropout(0.3),\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reshape(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(1, 2)  # For pooling\n",
    "        x = self.pooling(x).squeeze(-1)\n",
    "        return self.classifier(x)"
   ],
   "metadata": {
    "id": "3zssWz1LVg9E"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.max(1)[1]).sum().item()\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss / total)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / total:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Plotting Loss and Accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  with torch.no_grad():\n",
    "      for inputs, labels in val_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          val_loss += loss.item() * inputs.size(0)\n",
    "          _, predicted = outputs.max(1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels.max(1)[1]).sum().item()\n",
    "\n",
    "  val_loss /= total\n",
    "  val_acc = 100. * correct / total\n",
    "  return val_loss, val_acc\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "ibQ2rDvo5GCB",
    "ExecuteTime": {
     "end_time": "2025-01-24T18:29:23.039713Z",
     "start_time": "2025-01-24T18:29:23.032933Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "##############################全局参数#######################################\n",
    "f = h5py.File(r'dataset\\GOLD_XYZ_OSC.0001_1024.hdf5','r')\n",
    "dir_path = r'ExtractDataset'\n",
    "if os.path.exists(dir_path):\n",
    "    # Remove the directory\n",
    "    shutil.rmtree(dir_path)\n",
    "    print(f\"Deleted: {dir_path}\")\n",
    "    # Recreate the directory\n",
    "    os.mkdir(dir_path)\n",
    "modu_snr_size = 1200\n",
    "############################################################################\n",
    "\n",
    "for modu in range(24):\n",
    "\tX_list = []\n",
    "\tY_list = []\n",
    "\tZ_list = []\n",
    "\tprint('part ',modu)\n",
    "\tstart_modu = modu*106496\n",
    "\tfor snr in range(26):\n",
    "\t\tstart_snr = start_modu + snr*4096\n",
    "\t\tidx_list = np.random.choice(range(0,4096),size=modu_snr_size,replace=False)\n",
    "\t\tX = f['X'][start_snr:start_snr+4096][idx_list]\n",
    "\t\t#X = X[:,0:768,:]\n",
    "\t\tX_list.append(X)\n",
    "\t\tY_list.append(f['Y'][start_snr:start_snr+4096][idx_list])\n",
    "\t\tZ_list.append(f['Z'][start_snr:start_snr+4096][idx_list])\n",
    "\n",
    "\tfilename = dir_path + '/part' + str(modu) + '.h5'\n",
    "\tfw = h5py.File(filename,'w')\n",
    "\tfw['X'] = np.vstack(X_list)\n",
    "\tfw['Y'] = np.vstack(Y_list)\n",
    "\tfw['Z'] = np.vstack(Z_list)\n",
    "\tprint('X shape:',fw['X'].shape)\n",
    "\tprint('Y shape:',fw['Y'].shape)\n",
    "\tprint('Z shape:',fw['Z'].shape)\n",
    "\tfw.close()\n",
    "f.close()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "g_ES0PQrCtyQ",
    "outputId": "38fc73f2-c09a-4278-da0b-3989c1c7f013",
    "ExecuteTime": {
     "end_time": "2025-01-24T15:28:52.706869Z",
     "start_time": "2025-01-24T15:27:23.473402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ExtractDataset\n",
      "part  0\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  1\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  2\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  3\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  4\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  5\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  6\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  7\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  8\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  9\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  10\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  11\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  12\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  13\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  14\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  15\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  16\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  17\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  18\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  19\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  20\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  21\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  22\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  23\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSN92TkBOU9Z",
    "outputId": "b14182cb-3c1a-4d43-e01b-87301f176f90"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "# File paths and dataset split\n",
    "    drivePath = 'drive/MyDrive/Colab Notebooks/dataset/'\n",
    "    f = h5py.File(drivePath + r'part0.h5')\n",
    "    sample_num = f['X'].shape[0]\n",
    "    file_list = [drivePath + f\"part{i}.h5\" for i in range(1)]\n",
    "    idx = np.random.choice(range(0,sample_num),size=60000)\n",
    "\n",
    "    train_size = int(0.8 * len(idx))\n",
    "    train_idx, test_idx = idx[:train_size], idx[train_size:]\n",
    "\n",
    "    train_dataset = H5Dataset(file_list, train_idx)\n",
    "    test_dataset = H5Dataset(file_list, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ],
   "metadata": {
    "id": "ypaG3ZFs5JKI",
    "ExecuteTime": {
     "end_time": "2025-01-24T18:46:17.348792Z",
     "start_time": "2025-01-24T18:43:42.840670Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Model, Loss, Optimizer\n",
    "input_shape = train_dataset[0][0].shape\n",
    "num_classes = 24\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ProposedModel(input_shape, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs, labels\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        y_true.extend(labels.max(1)[1].cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, classes=[\"32PSK\", \"16APSK\", \"32QAM\", \"FM\", \"GMSK\", \"32APSK\", \"OQPSK\", \"8ASK\", \"BPSK\", \"8PSK\", \"AM-SSB-SC\", \"4ASK\", \"16PSK\", \"64APSK\", \"128QAM\", \"128APSK\", \"AM-DSB-SC\", \"AM-SSB-WC\", \"64QAM\", \"QPSK\", \"256QAM\", \"AM-DSB-WC\", \"OOK\", \"16QAM\"])"
   ],
   "metadata": {
    "id": "H0ZalfH7XYOp"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
