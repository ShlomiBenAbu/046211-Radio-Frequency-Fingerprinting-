{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import intel_npu_acceleration_library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "id": "BXyVK7IE3fvS",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-25T23:51:41.645084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orhar\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset Preparation\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, file_list, idx, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for file in file_list:\n",
    "            with h5py.File(file, 'r') as f:\n",
    "                self.data.append(f['X'][:][idx])\n",
    "                self.labels.append(f['Y'][:][idx])\n",
    "        self.data = np.vstack(self.data)\n",
    "        self.labels = np.vstack(self.labels)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ],
   "metadata": {
    "id": "8GzkY0OW3kmO",
    "ExecuteTime": {
     "end_time": "2025-01-25T19:24:14.423825Z",
     "start_time": "2025-01-25T19:24:14.412021Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Transformer Components\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query_dense = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_dense = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_dense = nn.Linear(embed_dim, embed_dim)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = torch.matmul(query, key.transpose(-2, -1))\n",
    "        dim_key = torch.tensor(key.shape[-1], dtype=torch.float32)\n",
    "        scaled_score = score / torch.sqrt(dim_key)\n",
    "        weights = torch.softmax(scaled_score, dim=-1)\n",
    "        output = torch.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.projection_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous()\n",
    "        concat_attention = attention.view(batch_size, -1, self.embed_dim)\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ],
   "metadata": {
    "id": "a4O15FmW4fyT",
    "ExecuteTime": {
     "end_time": "2025-01-25T19:24:15.636052Z",
     "start_time": "2025-01-25T19:24:15.597285Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ],
   "metadata": {
    "id": "XnIe8Lj64ljm",
    "ExecuteTime": {
     "end_time": "2025-01-25T19:24:17.435604Z",
     "start_time": "2025-01-25T19:24:17.411310Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "# Model Definition\n",
    "def proposed_model(X_train_shape, num_classes):\n",
    "    embed_dim = 1024  # Embedding size for each token\n",
    "    num_heads = 128  # Number of attention heads\n",
    "    ff_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.reshape = nn.Linear(X_train_shape[-1], 1024*2)\n",
    "            self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "            self.batch_norm = nn.BatchNorm1d(embed_dim)\n",
    "            self.dropout1 = nn.AlphaDropout(0.3)\n",
    "            self.dense1 = nn.Linear(embed_dim, 128)\n",
    "            self.dropout2 = nn.AlphaDropout(0.2)\n",
    "            self.dense2 = nn.Linear(128, 128)\n",
    "            self.dropout3 = nn.AlphaDropout(0.2)\n",
    "            self.output_layer = nn.Linear(128, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 2, 1024)\n",
    "            x = self.transformer_block(x)\n",
    "            x = self.global_avg_pool(x.transpose(1, 2)).squeeze(-1)\n",
    "            x = self.batch_norm(x)\n",
    "            x = self.dropout1(torch.selu(self.dense1(x)))\n",
    "            x = self.dropout2(torch.selu(self.dense2(x)))\n",
    "            x = self.dropout3(x)\n",
    "            x = torch.softmax(self.output_layer(x), dim=-1)\n",
    "            return x\n",
    "\n",
    "    return Model()"
   ],
   "metadata": {
    "id": "3zssWz1LVg9E",
    "ExecuteTime": {
     "end_time": "2025-01-25T19:24:18.736791Z",
     "start_time": "2025-01-25T19:24:18.710925Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.max(1)[1]).sum().item()\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss / total)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / total:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Plotting Loss and Accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  with torch.no_grad():\n",
    "      for inputs, labels in val_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          val_loss += loss.item() * inputs.size(0)\n",
    "          _, predicted = outputs.max(1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels.max(1)[1]).sum().item()\n",
    "\n",
    "  val_loss /= total\n",
    "  val_acc = 100. * correct / total\n",
    "  return val_loss, val_acc\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "# def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "#     disp.plot(cmap=plt.cm.Blues)\n",
    "#     plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ],
   "metadata": {
    "id": "ibQ2rDvo5GCB",
    "ExecuteTime": {
     "end_time": "2025-01-25T20:16:51.157017Z",
     "start_time": "2025-01-25T20:16:51.116264Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "##############################全局参数#######################################\n",
    "f = h5py.File(r'dataset\\GOLD_XYZ_OSC.0001_1024.hdf5','r')\n",
    "dir_path = r'ExtractDataset'\n",
    "if os.path.exists(dir_path):\n",
    "    # Remove the directory\n",
    "    shutil.rmtree(dir_path)\n",
    "    print(f\"Deleted: {dir_path}\")\n",
    "    # Recreate the directory\n",
    "    os.mkdir(dir_path)\n",
    "modu_snr_size = 1200\n",
    "############################################################################\n",
    "\n",
    "for modu in range(24):\n",
    "\tX_list = []\n",
    "\tY_list = []\n",
    "\tZ_list = []\n",
    "\tprint('part ',modu)\n",
    "\tstart_modu = modu*106496\n",
    "\tfor snr in range(26):\n",
    "\t\tstart_snr = start_modu + snr*4096\n",
    "\t\tidx_list = np.random.choice(range(0,4096),size=modu_snr_size,replace=False)\n",
    "\t\tX = f['X'][start_snr:start_snr+4096][idx_list]\n",
    "\t\t#X = X[:,0:768,:]\n",
    "\t\tX_list.append(X)\n",
    "\t\tY_list.append(f['Y'][start_snr:start_snr+4096][idx_list])\n",
    "\t\tZ_list.append(f['Z'][start_snr:start_snr+4096][idx_list])\n",
    "\n",
    "\tfilename = dir_path + '/part' + str(modu) + '.h5'\n",
    "\tfw = h5py.File(filename,'w')\n",
    "\tfw['X'] = np.vstack(X_list)\n",
    "\tfw['Y'] = np.vstack(Y_list)\n",
    "\tfw['Z'] = np.vstack(Z_list)\n",
    "\tprint('X shape:',fw['X'].shape)\n",
    "\tprint('Y shape:',fw['Y'].shape)\n",
    "\tprint('Z shape:',fw['Z'].shape)\n",
    "\tfw.close()\n",
    "f.close()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "g_ES0PQrCtyQ",
    "outputId": "38fc73f2-c09a-4278-da0b-3989c1c7f013",
    "ExecuteTime": {
     "end_time": "2025-01-25T19:46:54.426593Z",
     "start_time": "2025-01-25T19:46:23.623313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ExtractDataset\n",
      "part  0\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  1\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  2\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  3\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  4\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  5\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  6\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  7\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  8\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  9\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  10\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  11\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  12\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  13\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  14\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  15\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  16\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  17\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  18\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  19\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  20\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  21\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  22\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n",
      "part  23\n",
      "X shape: (31200, 1024, 2)\n",
      "Y shape: (31200, 24)\n",
      "Z shape: (31200, 1)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "# File paths and dataset split\n",
    "    batch_size=1024\n",
    "    # drivePath = 'drive/MyDrive/Colab Notebooks/dataset/'\n",
    "    f = h5py.File(dir_path + r'\\part0.h5')\n",
    "    sample_num = f['X'].shape[0]\n",
    "    file_list = [dir_path + f\"\\part{i}.h5\" for i in range(24)]\n",
    "    idx = np.random.choice(range(0,sample_num),size=60000)\n",
    "\n",
    "    train_size = int(0.8 * len(idx))\n",
    "    train_idx, test_idx = idx[:train_size], idx[train_size:]\n",
    "\n",
    "    train_dataset = H5Dataset(file_list, train_idx)\n",
    "    test_dataset = H5Dataset(file_list, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "id": "ypaG3ZFs5JKI",
    "ExecuteTime": {
     "end_time": "2025-01-25T21:12:57.173638Z",
     "start_time": "2025-01-25T21:07:11.688181Z"
    }
   },
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "source": [
    "# Model, Loss, Optimizer\n",
    "input_shape = train_dataset[0][0].shape\n",
    "num_classes = 24\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = proposed_model(input_shape, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100)\n",
    "\n",
    "# Confusion Matrix\n",
    "y_true = []\n",
    "y_pred = []\n",
    "outputs_test = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs, labels\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        outputs_test.extend(outputs.cpu().numpy())\n",
    "        y_true.extend(labels.max(1)[1].cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Plot confusion matrix\n",
    "classes=[\"32PSK\", \"16APSK\", \"32QAM\", \"FM\", \"GMSK\", \"32APSK\", \"OQPSK\", \"8ASK\", \"BPSK\", \"8PSK\", \"AM-SSB-SC\", \"4ASK\", \"16PSK\", \"64APSK\", \"128QAM\", \"128APSK\", \"AM-DSB-SC\", \"AM-SSB-WC\", \"64QAM\", \"QPSK\", \"256QAM\", \"AM-DSB-WC\", \"OOK\", \"16QAM\"]\n",
    "batch_size = 1024\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "outputs_test= np.array(outputs_test)\n",
    "for i in range(0, len(test_dataset)):\n",
    "    j = list(test_dataset.labels[i,:]).index(1)\n",
    "    k = int(np.argmax(outputs_test[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "plot_confusion_matrix(confnorm, labels=classes)\n",
    "# plot_confusion_matrix(y_true, y_pred, classes=[\"32PSK\", \"16APSK\", \"32QAM\", \"FM\", \"GMSK\", \"32APSK\", \"OQPSK\", \"8ASK\", \"BPSK\", \"8PSK\", \"AM-SSB-SC\", \"4ASK\", \"16PSK\", \"64APSK\", \"128QAM\", \"128APSK\", \"AM-DSB-SC\", \"AM-SSB-WC\", \"64QAM\", \"QPSK\", \"256QAM\", \"AM-DSB-WC\", \"OOK\", \"16QAM\"])"
   ],
   "metadata": {
    "id": "H0ZalfH7XYOp",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-25T21:13:10.059149Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
